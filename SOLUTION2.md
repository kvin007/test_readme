# Cloud-Based ETL Pipeline for Analyzing User Behavior and Song Insights at Sparkify

Sparkify, a rapidly growing music streaming startup, has experienced significant expansion in both their user base and song database. To leverage the benefits of cloud computing, Sparkify has decided to migrate their processes and data to the cloud. Currently, their data is stored in Amazon S3, with one directory containing JSON logs capturing user activity on the app, and another directory housing JSON metadata about the songs available on the platform.

This project extracts the relevant data from the S3 buckets, stages it in Amazon Redshift, and subsequently transforms it into a star schema. These tables will empower the analytics team at Sparkify to delve deeper into user behavior and gain valuable insights into the songs their users are listening to.


## Sample input data

The AWS bucket contains JSON data pertaining to songs and user activity on the app, which adheres to the following structure:

- **song_data**:

	The song dataset used in this project is a subset of real data from the Million Song Dataset. Each file in the dataset is in JSON format and contains metadata about a specific song and its corresponding artist. The files are organized and partitioned based on the first three letters of each song's track ID. Below are examples of file paths for two files within the dataset:
	```
		song_data/A/B/C/TRABCEI128F424C983.json
		song_data/A/A/B/TRAABJL12903CDCF1A.json
	```
	Here is an illustration of the structure and content of a single song file, TRAABJL12903CDCF1A.json:

	```json
	{
		"num_songs": 1,
		"artist_id": "ARJIE2Y1187B994AB7",
		"artist_latitude": null,
		"artist_longitude": null,
		"artist_location": "",
		"artist_name": "Line Renaud",
		"song_id": "SOUPIRU12A6D4FA1E1",
		"title": "Der Kleine Dompfaff",
		"duration": 152.92036,
		"year": 0
	}
	```
- **log_data**:

	The log dataset used in this project comprises log files in JSON format generated by an event simulator. These logs are based on the songs present in the song dataset described above. The log files simulate the activity logs of an imaginary music streaming application, reflecting user interactions and events based on specific configuration settings.

	The log files in the dataset are partitioned by year and month, allowing for efficient data retrieval and analysis. Here are examples of file paths for two files within this dataset:
	```
	log_data/2018/11/2018-11-12-events.json
	log_data/2018/11/2018-11-13-events.json
	```
	The log files contain information about various events and actions performed by users within the music streaming app. During the ETL process, these log files will be extracted, transformed, and loaded into staging tables within Redshift. The data will then be further processed and transformed to create dimensional tables that enable detailed analysis of user behavior and insights into the songs users are listening to.

		
## How to run the Python scripts?

This repository includes the following files with their respective explanations:

- **dwh.cfg**: A *configuration file* that specifies the S3 paths, IAM role, and cluster information needed for the ETL process.

- **sql_queries.py**: A Python file containing *SQL scripts* used for creating, dropping, and inserting data into the corresponding tables.

- **create_tables.py**: A Python script that executes the drop and create sql scripts for creating the neccesary tables.

- **etl.py**: A Python script that loads the data from the S3 buckets into Redshift's staging tables and corresponding dimension and fact tables.

### Sequence of steps for execution:

*Before running these scripts, it is necessary to have an active Redshift cluster in your AWS account. Make sure to modify the appropriate values in the dwh.cfg file to match your specific cluster configuration.*

a. Run the **create_tables.py** script as this part of the job drops and recreates any neccesary table
b. Run the **etl.py** script as this part of the job loads from S3 into the staging tables and populates the fact and dimension tables.

## Star Schema design

The proposed star schema consists of the following dimension and fact tables:

- **Fact tables**:
	- songplays (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

- **Dimension tables**:
	- users (user_id, first_name, last_name, gender, level)
	- songs (song_id, title, artist_id, year, duration)
	- artists (artist_id, name, location, latitude, longitude)
	- time (start_time, hour, day, week, month, year, weekday)

A star schema is an ideal choice for this project as it effectively represents the core process of the startup, which revolves around users listening to songs. This schema follows a denormalized structure, making it easier to query and comprehend for stakeholders and decision-makers. By organizing the data into fact and dimension tables, the star schema simplifies complex relationships and enables efficient analysis of user behavior and song preferences.
